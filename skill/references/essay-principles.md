<table_of_contents>
1. Overview
2. Principle 1: Authorship Preservation
3. Principle 2: Creative Range
4. Principle 3: Voice Amplification
5. Principle 4: Process Involvement
6. Principle 5: Depth of Exploration
7. Principle 6: Input Agency
8. Applying the Principles
</table_of_contents>

<overview>
The six Dignified Technology principles are derived from the essay "What Does a Tool Owe You?" They define what it means for a tool to protect and elevate the creative, expressive, and irreplaceable dimensions of human work.

Each principle corresponds to a **dignity metric** — a measurable quality that determines whether a tool deepens the user's relationship to their work or erodes it.

The core thesis: **"A tool owes you your own capability back, amplified."**
</overview>

<principle number="1" name="Authorship Preservation">
<question>"Would the user sign their name on the work output without hesitation?"</question>

<essay_grounding>
The essay contrasts two paths: "the human becomes an editor of machine output rather than an author" versus a tool that helps the user go deeper so "the work you produce feels genuinely yours."

Authorship is not about who typed the words. It is about whether the user shaped the thinking, made the decisions, and owns the result as an expression of their intent.
</essay_grounding>

<what_to_look_for>
- Does the tool produce drafts the user merely accepts, or does it scaffold the user's own creation?
- When AI generates content, is the user's role reduced to approving/editing, or do they remain the originator?
- Can the user trace the output back to their own decisions and thinking?
- Would the user feel comfortable claiming full authorship of the result?
</what_to_look_for>

<positive_signals>
- User writes their own content with AI providing structure, feedback, or research
- AI generates options the user selects from and builds upon
- The tool explicitly positions the user as author and itself as assistant
- Output requires substantial user input and decision-making to produce
</positive_signals>

<negative_signals>
- Prompt-to-output pipeline where the user's main action is accepting or lightly editing
- AI-generated content presented as final or near-final
- User's contribution is reduced to selecting from AI-generated options
- The tool's voice dominates the output
</negative_signals>
</principle>

<principle number="2" name="Creative Range">
<question>"Does this tool expand the user's creative range, or flatten it?"</question>

<essay_grounding>
The essay asks whether a tool helps users explore "more connections made, more assumptions challenged, more concepts explored." A tool that expands creative range surfaces possibilities the user wouldn't have found alone. A tool that flattens it converges to a single generated output.
</essay_grounding>

<what_to_look_for>
- Does the tool surface multiple angles, approaches, or framings?
- Does it help the user explore a wider space of ideas before converging?
- Or does it collapse to a single "best" output immediately?
- Does it expose the user to unexpected connections or perspectives?
</what_to_look_for>

<positive_signals>
- Tool surfaces 3+ angles, counterarguments, or alternative approaches
- Divergent exploration before convergent execution
- Users report discovering ideas they wouldn't have found alone
- The tool challenges the user's initial framing
</positive_signals>

<negative_signals>
- Single output with no alternatives shown
- Immediate convergence to "the answer"
- Template-driven output that looks the same regardless of input
- No mechanism for exploring adjacent or contrarian ideas
</negative_signals>
</principle>

<principle number="3" name="Voice Amplification">
<question>"Does it amplify their voice, or replace it?"</question>

<essay_grounding>
The essay distinguishes between tools that help "you go further into the idea than you could have gone alone" (amplification) and tools where "the relationship between the person and their ideas is replaced by a relationship between the person and a tool's output" (replacement).

Voice is not just writing style — it is the user's perspective, priorities, values, and way of seeing the world.
</essay_grounding>

<what_to_look_for>
- Does the output reflect the user's thinking, style, and perspective?
- Does the tool adapt to the user rather than imposing its own defaults?
- Is the user's unique perspective preserved and strengthened in the output?
- Or does everything produced by the tool sound the same regardless of who uses it?
</what_to_look_for>

<positive_signals>
- Output varies significantly based on who is using the tool
- Tool learns from or adapts to the user's style and preferences
- User's prior work informs AI behavior
- The tool asks about the user's perspective before generating
</positive_signals>

<negative_signals>
- Generic, default-tone output regardless of user
- Tool's "voice" overwrites the user's perspective
- No mechanism for expressing or preserving user style
- Output could have been produced by anyone using the same prompt
</negative_signals>
</principle>

<principle number="4" name="Process Involvement">
<question>"Did the tool ask them questions, or just produce output?"</question>

<essay_grounding>
The essay describes the dignified path as one where "the tool asks questions that draw out what you actually think." Process involvement means the user is an active participant in creation, not a passive recipient of generated output.

The essay frames this as the difference between "designed to produce output" and "designed to deepen thinking."
</essay_grounding>

<what_to_look_for>
- Is the user actively involved at each stage of the creation process?
- Does the tool ask questions, request clarification, or prompt reflection?
- Or is the flow: prompt -> output -> accept?
- Are there meaningful decision points where the user shapes the direction?
</what_to_look_for>

<positive_signals>
- Multi-step process with user input at each stage
- Tool asks clarifying questions before producing output
- User makes directional choices throughout the workflow
- Iterative collaboration rather than one-shot generation
</positive_signals>

<negative_signals>
- Single prompt -> single output workflow
- No intermediate steps where the user can steer
- User's only interaction is accepting or rejecting final output
- The tool "does everything" without checking in
</negative_signals>
</principle>

<principle number="5" name="Depth of Exploration">
<question>"Did they reach an insight they wouldn't have found alone?"</question>

<essay_grounding>
The essay describes a tool that "challenges the weakest assumption in the draft" and "finds connections to other work you've done and forgotten about." Depth of exploration is about whether the tool helps the user think more deeply — not just faster.

The essay's core claim: "The transformation is in the process, not the outcome." A tool that short-circuits thinking by providing ready answers removes the very thing that makes the work valuable.
</essay_grounding>

<what_to_look_for>
- Does the tool help users surface assumptions, find connections, or challenge their thinking?
- Does it encourage deeper engagement with the material?
- Or does it provide ready answers that bypass the thinking process?
- Does the user learn something through using the tool?
</what_to_look_for>

<positive_signals>
- Tool surfaces assumptions the user hadn't examined
- Connections drawn to the user's own prior work or thinking
- Socratic questioning that deepens understanding
- Users report learning or discovering insights during the process
</positive_signals>

<negative_signals>
- Ready-made answers that bypass the thinking process
- No mechanism for challenging or deepening the user's ideas
- Output that is "good enough" but shallow
- Users report saving time but not gaining understanding
</negative_signals>
</principle>

<principle number="6" name="Input Agency">
<question>"Your capacity to govern what reaches you."</question>

<essay_grounding>
The essay introduces "input agency" as "your capacity to govern what reaches you" — distinct from the usual sense of agency as capacity to act. It argues that "most tools we build today have been designed to maximize other people's access to our attention while giving us almost no sovereignty over what enters our field."

The essay is explicit: "A tool that erodes your input agency is already failing the dignity test before you've even started using it for its stated purpose."
</essay_grounding>

<what_to_look_for>
- Does the user have sovereignty over what the tool presents to them?
- Can the user control notifications, suggestions, and AI-initiated content?
- Is the default respectful of attention, or does the user have to opt out?
- Does the tool insert itself into the user's attention without consent?
</what_to_look_for>

<positive_signals>
- User explicitly requests AI assistance (opt-in, not opt-out)
- Minimal or no unsolicited notifications, suggestions, or interruptions
- Clear controls over what the tool surfaces and when
- Default state is quiet; the tool activates when asked
</positive_signals>

<negative_signals>
- Unsolicited AI suggestions or auto-completions
- Notifications the user didn't ask for
- Algorithmic content insertion without consent
- Opting out requires more effort than opting in
</negative_signals>
</principle>

<applying_principles>
When evaluating a product against these principles, look for **evidence in code, design, and documentation** — not just intent. The question is not "do the builders care about dignity?" but "does the product, as built, protect it?"

Key sources of evidence:
- **AI integration points**: Where does AI generate content? What is the user's role at each point?
- **User-facing flows**: How many steps involve user input vs. AI output?
- **Controls and settings**: What can the user configure about AI behavior?
- **Defaults**: What happens if the user does nothing? Is the default dignified?
- **Documentation/PRDs**: Do product requirements mention user agency, authorship, or values?
</applying_principles>
